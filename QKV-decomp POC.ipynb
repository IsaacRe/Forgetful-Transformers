{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa24bdcb-cfe5-4e1b-aa1c-4c15a0fc965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381c1223-c295-42f4-a848-75fa3d6fdfaf",
   "metadata": {},
   "source": [
    "### Load Q, K, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94278971-0668-4fed-a4e1-4bd29861bde5",
   "metadata": {},
   "source": [
    "Generate random test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d3c0b69a-05eb-47ae-a5fe-7007f8d2085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "psd_ratio = 1  # test how error increases as Q becomes less PSD\n",
    "d = 64\n",
    "e = 400\n",
    "l = 400\n",
    "batch = 10\n",
    "seed = 12\n",
    "assert l >= d\n",
    "if seed:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "arange_d = torch.arange(d, 0, -1)\n",
    "psd_matrix = torch.Tensor(np.random.rand(d, d))\n",
    "psd_matrix = torch.matmul(psd_matrix, psd_matrix)\n",
    "psd_matrix = torch.cat([psd_matrix, torch.ones(l - d, d)], dim=0)\n",
    "q = torch.Tensor(np.random.rand(l, d)) * (1 - psd_ratio) + psd_matrix * psd_ratio\n",
    "k = torch.Tensor(np.random.rand(l, d)) * (1 - psd_ratio) + psd_matrix * psd_ratio\n",
    "\n",
    "# make sure query vectors are not all zero (creates large error when solving for K_hat, and will never occur in practice)\n",
    "\n",
    "q, k = q.type(torch.float), k.type(torch.float)\n",
    "v = torch.Tensor(np.random.rand(l, d))\n",
    "\n",
    "q, k, v = q[None].repeat(batch, 1, 1), k[None].repeat(batch, 1, 1), v[None].repeat(batch, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065207c4-b0e6-4efd-98f7-05066e374f90",
   "metadata": {},
   "source": [
    "Or load q, k, v from GPT-2 run on WMT (see end of notebook for data collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6178177e-bf03-4567-9d36-7f18f2f082d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 400\n",
    "\n",
    "qkv = np.load('qkv.npz')\n",
    "n_layer, n_sample, n_attn_heads, l, d = qkv['q'].shape  # [ layers X samples X attention heads X tokens X q dimension ]\n",
    "\n",
    "sample_dims = np.random.randint(0, n_sample * n_attn_heads, (n_layer,))\n",
    "sample_mask = torch.zeros(n_layer, n_sample * n_attn_heads).type(torch.bool)\n",
    "sample_mask[np.arange(n_layer), sample_dims] = True\n",
    "\n",
    "# sample along sample and attention head dimensions\n",
    "q, k, v = (\n",
    "    torch.Tensor(qkv[n]).reshape(n_layer, n_sample * n_attn_heads, l, d)[sample_mask]\n",
    "    for n in ('q', 'k', 'v')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0e645-cf20-46cf-9797-d46cda2d0600",
   "metadata": {},
   "source": [
    "### Run decomposition\n",
    "Approximate within-softmax QK decomposition to bring dimensionality reduction outside softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "82ff216f-42ba-4511-a513-02ebfb933175",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.softmax(q @ k.transpose(-1, -2), dim=-1)\n",
    "out = A @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "027ed76a-84cf-42dc-bb89-6e58fc29e85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9609e-05, 2.1393e-05, 3.9042e-05, 2.2519e-05, 2.5699e-05, 2.6788e-05,\n",
       "        3.5920e-05, 4.3862e-05, 2.0114e-05, 2.6681e-05, 2.4320e-05, 3.9945e-05])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decompose A into USD\n",
    "U, s, D = torch.linalg.svd(A)\n",
    "S = torch.diag_embed(s, dim1=1, dim2=2)\n",
    "\n",
    "# take highest singular value vectors\n",
    "U = U[:,:,:e]\n",
    "S = S[:,:e,:e]\n",
    "D = D[:,:e,:]\n",
    "\n",
    "((A - U @ S @ D) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "05d6ce37-954b-4b65-a029-c63d4affd9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.4222e-05, 1.7222e-04, 5.0951e-05, 4.6841e-05, 4.4781e-05, 1.0790e-04,\n",
       "        1.3410e-04, 8.2946e-05, 5.1122e-05, 6.8018e-05, 7.2581e-05, 7.6963e-05])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute pseudo A\n",
    "US = U @ S\n",
    "\n",
    "# compute log offset\n",
    "offset = 1 - US.min(dim=-1).values.min(dim=-1).values\n",
    "US_ = US + offset[:,None,None]\n",
    "M_offset = torch.ones_like(US_) * offset[:,None,None]\n",
    "# M_offset = torch.linalg.lstsq(US_, US).solution  # doesnt work\n",
    "\n",
    "# make rows of pseudo A sum to 1\n",
    "rowsum = US_.sum(dim=-1)\n",
    "A_hat = US_ / rowsum[:,:,None]\n",
    "\n",
    "# compute V_hat\n",
    "V_hat = D @ v\n",
    "output_offset = -(M_offset @ D @ v)\n",
    "((A - (US_ @ D - M_offset @ D)) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "1ded2ee9-2010-4c90-b0d8-f7794a5fe87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([298, 205,  95, 102,  73, 136, 143,  74, 110,  72, 115,  69]),\n",
       " tensor([298, 205,  95, 102,  73, 136, 143,  74, 110,  72, 115,  69]),\n",
       " tensor([242, 122,  72,  71,  54,  83,  94,  39,  71,  41,  78,  37]),\n",
       " tensor([246, 131,  74,  75,  57,  89, 101,  42,  78,  44,  81,  42]))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.matrix_rank(A), torch.linalg.matrix_rank(US), torch.linalg.matrix_rank(US_), torch.linalg.matrix_rank(torch.log(US_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a7308059-520e-469d-988b-32fd2438350d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27.4845, 23.1989, 27.0541, 20.6320, 20.7098, 17.6928, 17.4312, 17.5956,\n",
       "        21.0654, 18.8198, 17.8044, 15.4002])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solve for K_hat - may find different solutions to same q, US_ due to randomness (even after seeding)\n",
    "K_hat = torch.linalg.lstsq(q, torch.log(US_)).solution.transpose(-1, -2)\n",
    "((torch.log(US_) - q @ K_hat.transpose(-1, -2)) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5  # overdetermined system when l > d (always)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6355d2ee-7331-4b8d-82cc-c2a4778e877b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0185, 0.0164, 0.0173, 0.0165, 0.0169, 0.0137, 0.0141, 0.0072, 0.0178,\n",
       "        0.0141, 0.0164, 0.0087])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recompute A prime\n",
    "A_hat_p = torch.softmax(q @ K_hat.transpose(-1, -2), dim=-1)\n",
    "((A_hat - A_hat_p) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "cfbf8689-8522-427e-a558-0a0d483fd349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
       "         0.0003, 0.0003, 0.0003]),\n",
       " tensor([14.7886, 13.1058, 13.8717, 13.2007, 13.4920, 10.9209, 11.2509,  5.7252,\n",
       "         14.2186, 11.2537, 13.0806,  6.9801]),\n",
       " tensor([14.7886, 13.1058, 13.8717, 13.2007, 13.4920, 10.9209, 11.2509,  5.7252,\n",
       "         14.2186, 11.2537, 13.0806,  6.9801]))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconstruct original attention matrix\n",
    "A_p = (A_hat @ D) * rowsum[:,:,None] - M_offset @ D\n",
    "A_pp = (A_hat_p @ D) * rowsum[:,:,None] - M_offset @ D\n",
    "(\n",
    "    ((A - A_p) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5,\n",
    "    ((A - A_pp) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5,\n",
    "    ((A_p - A_pp) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "1bff4c5e-7993-4755-aef2-22a5a3fde12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.3264e-05, 7.3146e-05, 6.9990e-05, 7.1059e-05, 7.3563e-05, 1.1071e-04,\n",
       "         9.4446e-05, 1.5011e-04, 1.1413e-04, 1.5240e-04, 1.7317e-04, 1.6475e-04]),\n",
       " tensor([0.9898, 2.4709, 1.7239, 1.4547, 1.9608, 2.1588, 2.6071, 1.1689, 3.4130,\n",
       "         3.1450, 5.5249, 2.1639]),\n",
       " tensor([0.9898, 2.4709, 1.7239, 1.4547, 1.9608, 2.1588, 2.6071, 1.1689, 3.4130,\n",
       "         3.1450, 5.5249, 2.1639]))"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute new outputs\n",
    "out_p = (A_hat @ V_hat) * rowsum[:,:,None] + output_offset\n",
    "out_pp = (A_hat_p @ V_hat) * rowsum[:,:,None] + output_offset\n",
    "(\n",
    "    (((out - out_p) ** 2).sum(dim=-1) ** 0.5).mean(dim=-1),\n",
    "    (((out - out_pp) ** 2).sum(dim=-1) ** 0.5).mean(dim=-1),\n",
    "    (((out_p - out_pp) ** 2).sum(dim=-1) ** 0.5).mean(dim=-1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "e8395d63-b0f9-4b71-bf1a-a596de960ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.1358, 3.6509, 2.5654, 3.0447, 3.2976, 3.0497, 2.9470, 0.9034, 5.4433,\n",
       "        4.7898, 8.5961, 2.4983])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((out) ** 2).sum(dim=-1) ** 0.5).mean(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3517a2-3f3d-4501-a756-a20232cf1377",
   "metadata": {},
   "source": [
    "### Take 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "e83467fe-598b-4943-aaa9-6feade1e9989",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = torch.tril(torch.ones(A.shape[-2:]).type(torch.bool))\n",
    "A = torch.softmax(q @ k.transpose(-1, -2), dim=-1)\n",
    "#A[...,~attn_mask] = 0\n",
    "out = A @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "484e6b11-bd59-49f6-ba86-206702a29516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9609e-05, 2.1393e-05, 3.9042e-05, 2.2519e-05, 2.5699e-05, 2.6788e-05,\n",
       "        3.5920e-05, 4.3862e-05, 2.0114e-05, 2.6681e-05, 2.4320e-05, 3.9945e-05])"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decompose A into USD\n",
    "U, s, D = torch.linalg.svd(A)\n",
    "S = torch.diag_embed(s, dim1=1, dim2=2)\n",
    "\n",
    "# take highest singular value vectors\n",
    "U = U[:,:,:e]\n",
    "S = S[:,:e,:e]\n",
    "D = D[:,:e,:]\n",
    "\n",
    "((A - U @ S @ D) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "198f116a-1faf-46cd-aaff-7f9012def4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0832e-05, 1.2165e-04, 4.4922e-05, 3.6562e-05, 3.6650e-05, 8.1338e-05,\n",
       "        9.3193e-05, 6.2302e-05, 3.5899e-05, 4.8376e-05, 5.6828e-05, 5.9304e-05])"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute pseudo A\n",
    "US = U @ S\n",
    "\n",
    "# compute log offset\n",
    "offset = 1.3\n",
    "US_ = US + offset\n",
    "M_offset = torch.ones_like(US_) * offset\n",
    "# M_offset = torch.linalg.lstsq(US_, US).solution  # doesnt work\n",
    "\n",
    "# make rows of pseudo A sum to 1\n",
    "rowsum = US_.sum(dim=-1)\n",
    "A_hat = US_ / rowsum[:,:,None]\n",
    "\n",
    "# compute V_hat\n",
    "V_hat = D @ v\n",
    "output_offset = -(M_offset @ D @ v)\n",
    "((A - (US_ @ D - M_offset @ D)) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "a2879143-a884-4ab3-85cc-eeae13a0ea61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([298, 205,  95, 102,  73, 136, 143,  74, 110,  72, 115,  69]),\n",
       " tensor([298, 205,  95, 102,  73, 136, 143,  74, 110,  72, 115,  69]),\n",
       " tensor([246, 131,  73,  74,  56,  88, 102,  42,  79,  44,  80,  42]),\n",
       " tensor([260, 156,  84,  87,  63, 107, 116,  54,  88,  54,  92,  52]))"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.matrix_rank(A), torch.linalg.matrix_rank(US), torch.linalg.matrix_rank(US_), torch.linalg.matrix_rank(torch.log(US_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "be481148-3cad-4e46-b727-a546e7d55468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17.6073, 15.0833, 17.5408, 13.9145, 15.2176, 13.6837, 14.0984,  9.5459,\n",
      "        16.0747, 15.3922, 14.6954,  9.6832])\n"
     ]
    }
   ],
   "source": [
    "K_hat = torch.linalg.lstsq(q, torch.log(US_)).solution.transpose(-1, -2)\n",
    "print(((torch.log(US_) - q @ K_hat.transpose(-1, -2)) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620b399-8d79-48b8-804e-b163facbc64a",
   "metadata": {},
   "source": [
    "### Aside - try directly optimizging k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "6575a892-b9ee-4984-b1c3-0a174d149a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28acd889-2513-41ea-85da-efc7f2cd09f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_hat = torch.nn.Parameter(torch.zeros_like(k))\n",
    "V_hat = torch.nn.Parameter(torch.zeros_like(v))\n",
    "K_hat.data.normal_()\n",
    "V_hat.data.normal_()\n",
    "\n",
    "n_steps = 10000\n",
    "optim = torch.optim.SGD([K_hat, V_hat], lr=0.2)\n",
    "for i in (pbar := tqdm(range(n_steps))):\n",
    "    loss = (((torch.softmax(q @ K_hat.transpose(-2, -1), dim=-1) @ V_hat) - out) ** 2).sum(dim=-1).mean()\n",
    "    pbar.set_description(f\"loss: {loss.item()}\")\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "7c5d6d25-71a7-4671-9dee-3881e905f27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4077, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((out - (torch.softmax(q @ K_hat.transpose(-2, -1), dim=-1) @ V_hat)) ** 2).sum(dim=-1) ** 0.5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "62df932b-88ff-42e4-afc2-b687bf8d8595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2162, 0.5490, 0.0219, 0.0767, 0.0740, 0.3159, 0.4229, 0.1210, 0.2291,\n",
      "        0.6276, 0.1438, 1.0947])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(((((out - (torch.softmax(q @ K_hat.transpose(-2, -1), dim=-1) @ V_hat)) ** 2).sum(dim=-1) ** 0.5).mean(dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b7087-e1b4-470d-a880-02efb133b4dc",
   "metadata": {},
   "source": [
    "### Try solving for original V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "c096fccf-b8d9-4ae0-991e-69979f54e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.softmax(q @ k.transpose(-1, -2), dim=-1)\n",
    "#A[...,~attn_mask] = 0\n",
    "out = A @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "73b9cf5e-f0f2-4671-894b-99fed9ed2500",
   "metadata": {},
   "outputs": [
    {
     "ename": "_LinAlgError",
     "evalue": "torch.linalg.solve: (Batch element 4): The solver failed because the input matrix is singular.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[381], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m v__ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msolution\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: torch.linalg.solve: (Batch element 4): The solver failed because the input matrix is singular."
     ]
    }
   ],
   "source": [
    "v__ = torch.linalg.solve(A, out).solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "807eaed7-bab1-471a-96f2-6e89c75a0b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1046, 3.6361, 2.5274, 3.0096, 3.0790, 3.1536, 2.8280, 0.6648, 5.4319,\n",
      "        4.6320, 8.5844, 1.8041])\n"
     ]
    }
   ],
   "source": [
    "print((((out - A @ v__) ** 2).sum(dim=-1) ** 0.5).mean(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "b530ac36-e382-4b1a-895d-ac47115bf4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9714, 0.4000, 0.0071, 0.0422, 0.0416, 0.2649, 0.3864, 0.3217, 0.1348,\n",
      "        0.7272, 0.0907, 1.4685])\n"
     ]
    }
   ],
   "source": [
    "print((((out) ** 2).sum(dim=-1) ** 0.5).mean(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "59d335c3-efd5-4345-a48a-24663940b3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f156d605c9324b649fa43c8974f8847b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "V_hat = torch.nn.Parameter(torch.zeros_like(v))\n",
    "V_hat.data.normal_()\n",
    "\n",
    "n_steps = 10000\n",
    "optim = torch.optim.SGD([V_hat], lr=0.2)\n",
    "for i in (pbar := tqdm(range(n_steps))):\n",
    "    loss = (((A @ V_hat) - out) ** 2).sum(dim=-1).mean()\n",
    "    pbar.set_description(f\"loss: {loss.item()}\")\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "e3718141-7361-4709-88ac-8d05905f5679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8189, 0.9440, 0.4758, 0.5052, 0.4074, 0.7642, 0.8565, 0.3587, 0.4798,\n",
      "        0.4630, 0.8575, 0.4125])\n"
     ]
    }
   ],
   "source": [
    "print((((out - A @ V_hat) ** 2).sum(dim=-1) ** 0.5).mean(dim=-1).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70b9157-2e91-4302-8ddc-59133b40136b",
   "metadata": {},
   "source": [
    "### Try solving for V_hat after within-softmax decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3138b9e5-7e61-4948-85be-3e4631f5f79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d94c778d-57bf-4ec5-b269-563b2edaf01a",
   "metadata": {},
   "source": [
    "### Continue decomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "ff0bfc19-e2a1-4dfe-a291-d3f10b3c4364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21.6346, 17.7610, 22.2047, 16.8955, 20.1448, 18.2865, 19.0530, 11.2748,\n",
      "        21.1698, 21.8627, 19.5552, 12.5265])\n",
      "tensor([18.3572, 15.5771, 18.5209, 14.5647, 16.5345, 15.0256, 15.5906,  9.5470,\n",
      "        17.5020, 17.3379, 16.1797, 10.2331])\n",
      "tensor([17.6073, 15.0833, 17.5408, 13.9145, 15.2176, 13.6837, 14.0984,  9.5459,\n",
      "        16.0747, 15.3922, 14.6954,  9.6832])\n",
      "tensor([18.0905, 15.5026, 17.8915, 14.1389, 15.0216, 13.3313, 13.6019, 10.3054,\n",
      "        15.7617, 14.7003, 14.1734,  9.9381])\n"
     ]
    }
   ],
   "source": [
    "# solve for K_hat - may find different solutions to same q, US_ due to randomness (even after seeding)\n",
    "# overdetermined system when l > d (always) - the larger the offset the worse the solution\n",
    "# optimal offset around 1.3 (balance pos/neg post-log values)\n",
    "xs = np.arange(0.1, 0.5, 0.1)\n",
    "for x in xs:\n",
    "    K_hat = torch.linalg.lstsq(q, torch.log(US + 1 + x)).solution.transpose(-1, -2)\n",
    "    print(((torch.log(US + 1 + x) - q @ K_hat.transpose(-1, -2)) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "eb53dc3c-0b97-4e3c-9f71-824e11ca47af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0236, 0.0096, 0.0018, 0.0065, 0.0039, 0.0104, 0.0137, 0.0076, 0.0074,\n",
       "        0.0147, 0.0082, 0.0105])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recompute A prime\n",
    "A_hat_p = torch.softmax(q @ K_hat.transpose(-1, -2), dim=-1)\n",
    "((A_hat - A_hat_p) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "bfa1f010-2159-49b6-ba1b-25dc61831027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0001, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
       "         0.0002, 0.0002, 0.0002]),\n",
       " tensor([12.2878,  4.9760,  0.9185,  3.3771,  2.0346,  5.3996,  7.1118,  3.9255,\n",
       "          3.8505,  7.6451,  4.2630,  5.4825]),\n",
       " tensor([12.2878,  4.9760,  0.9185,  3.3771,  2.0346,  5.3996,  7.1118,  3.9255,\n",
       "          3.8505,  7.6451,  4.2630,  5.4825]))"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconstruct original attention matrix\n",
    "A_p = (A_hat @ D) * rowsum[:,:,None] - M_offset @ D\n",
    "A_pp = (A_hat_p @ D) * rowsum[:,:,None] - M_offset @ D\n",
    "(\n",
    "    ((A - A_p) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5,\n",
    "    ((A - A_pp) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5,\n",
    "    ((A_p - A_pp) ** 2).sum(dim=-1).sum(dim=-1) ** 0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "876b8d6e-2250-44ec-877a-cae3087891ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.7750e-05, 4.2608e-05, 9.9115e-05, 4.0284e-05, 4.5616e-05, 7.7316e-05,\n",
       "         6.4753e-05, 8.6857e-05, 6.2070e-05, 9.1452e-05, 1.0988e-04, 1.2083e-04]),\n",
       " tensor([0.9489, 0.8771, 0.0253, 0.1572, 0.2246, 0.3211, 0.7626, 0.1703, 0.4477,\n",
       "         1.1651, 0.2970, 1.0703]),\n",
       " tensor([0.9489, 0.8771, 0.0253, 0.1572, 0.2246, 0.3211, 0.7626, 0.1703, 0.4477,\n",
       "         1.1651, 0.2970, 1.0703]))"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute new outputs\n",
    "out_p = (A_hat @ V_hat) * rowsum[:,:,None] + output_offset\n",
    "out_pp = (A_hat_p @ V_hat) * rowsum[:,:,None] + output_offset\n",
    "(\n",
    "    (((out - out_p) ** 2).sum(dim=-1) ** 0.5).mean(dim=-1),\n",
    "    (((out - out_pp) ** 2).sum(dim=-1) ** 0.5).mean(dim=-1),\n",
    "    (((out_p - out_pp) ** 2).sum(dim=-1) ** 0.5).mean(dim=-1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "9f75be31-38c8-4412-acdc-d238140aa671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9714, 0.4000, 0.0071, 0.0422, 0.0416, 0.2649, 0.3864, 0.3217, 0.1348,\n",
       "        0.7272, 0.0907, 1.4685])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((out) ** 2).sum(dim=-1) ** 0.5).mean(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e036c4-e057-44e8-990f-37905f9f17bf",
   "metadata": {},
   "source": [
    "### Potential issues\n",
    "- Recomputing softmax for A_pp without original scaling factor of A_p\n",
    "- Scaling in alpha and inverse alpha at different points where they dont cancel\n",
    "- All-zero query vectors will make solving for K_hat ineffective\n",
    "- Q x K_hat = A_hat is overdetermined for K_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78396596-2e2b-43a8-a1ae-8ec9a530ae22",
   "metadata": {},
   "source": [
    "### Collect Q, K, V from WMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e71ac0f3-db81-4c12-8063-052690a0b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "import numpy as  np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import transformers_drop_in as drop_in\n",
    "import tensor_util as tu\n",
    "from config import CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef08ad30-8c90-49d1-a6ee-6be515144c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG.do_consolidate = True\n",
    "CONFIG.consolidate_ratio = 0.5\n",
    "CONFIG.context_length = 400\n",
    "CONFIG.consolidate_length = 200\n",
    "CONFIG.temperature = 0.1\n",
    "CONFIG.fix_prune_rate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fad639a-0b9e-49a4-ac1f-ed778d8cfc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "dataset = load_dataset('wikitext', 'wikitext-103-v1')\n",
    "split = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024d48d1-ae7a-4aad-80ef-208dfb60c2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = np.load('token_length.npy')\n",
    "all([len(tok) > 512 for tok in tokenizer.batch_encode_plus([split[int(i)]['text'] for i in np.where(l > 512)[0][:20]])['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a78f50-914d-41a7-94ad-d694ef44f4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2AttentionDropIn(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(CONFIG.device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c386258d-c93e-474d-8653-ab978c1c627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "indices, = np.where(l > CONFIG.context_length)\n",
    "batch_iter = iter(np.array_split(np.random.choice(indices, len(indices), replace=False), len(indices) // batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4bef317-d5fc-4df1-824c-ee58d327b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 12\n",
    "drop_in.GLOBALS.outputs = {\n",
    "    'q': [[] for _ in range(n_layer)],\n",
    "    'k': [[] for _ in range(n_layer)],\n",
    "    'v': [[] for _ in range(n_layer)],\n",
    "}\n",
    "\n",
    "def record_attn(layer_idx, query, key, value, unnormalized_attn, final_attn, attn_output, attn_mask):\n",
    "    drop_in.GLOBALS.outputs['q'][layer_idx] += [query.cpu()]\n",
    "    drop_in.GLOBALS.outputs['k'][layer_idx] += [key.cpu()]\n",
    "    drop_in.GLOBALS.outputs['v'][layer_idx] += [value.cpu()]\n",
    "\n",
    "\n",
    "def no_op(query, key, value, attn_weights):\n",
    "    pass\n",
    "\n",
    "drop_in.record_attn_vars = record_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f621d67-e470-41b3-b91c-54f43ab9641e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88752e9844a24a61aeb77b5989ee5cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_sample = 20\n",
    "n_layer = 12\n",
    "cols = 4\n",
    "rows = n_layer // cols\n",
    "rank_by_layer = [[] for _ in range(n_layer)]\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(n_sample)):\n",
    "        batch = next(batch_iter)\n",
    "        model_input = {name: t.to(CONFIG.device) for name, t in tokenizer.batch_encode_plus(split[batch]['text'],\n",
    "                                                                                         return_tensors=\"pt\",\n",
    "                                                                                         truncation=True,\n",
    "                                                                                         max_length=CONFIG.context_length).items()}\n",
    "        model(**model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4960fe50-6dbc-49bf-9137-bda4dc132431",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv = {\n",
    "    'q': [None for _ in range(n_layer)],\n",
    "    'k': [None for _ in range(n_layer)],\n",
    "    'v': [None for _ in range(n_layer)],\n",
    "}\n",
    "for m in ['q', 'k', 'v']:\n",
    "    for i in range(n_layer):\n",
    "        qkv[m][i] = torch.cat(drop_in.GLOBALS.outputs[m][i], dim=0)\n",
    "    qkv[m] = np.stack(qkv[m], axis=0)\n",
    "np.savez('qkv.npz', **qkv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da44da-12f0-4f5c-8a86-7706076b0c09",
   "metadata": {},
   "source": [
    "### SVD from eigen decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "8d2f51c8-628f-4116-a972-4e1832f06346",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, D = torch.linalg.svd(A)\n",
    "val, vec = torch.linalg.eig(A @ A.transpose(-1, -2))\n",
    "val_, vec_ = torch.linalg.eig(A.transpose(-1, -2) @ A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5071a6a5-75e8-44b2-bdbe-1f06dc3675c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([16.6662, 15.0797, 14.3649, 13.5416, 12.0955, 10.6551,  8.5561,  7.0162,\n",
       "          6.6288,  5.9499]),\n",
       " tensor([16.6662+0.j, 15.0797+0.j, 14.3649+0.j, 13.5416+0.j, 12.0956+0.j, 10.6551+0.j,\n",
       "          8.5561+0.j,  7.0162+0.j,  6.6288+0.j,  5.9499+0.j]),\n",
       " tensor([16.6662+0.j, 15.0797+0.j, 14.3649+0.j, 13.5416+0.j, 12.0956+0.j, 10.6551+0.j,\n",
       "          8.5561+0.j,  7.0162+0.j,  6.6288+0.j,  5.9500+0.j]))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0][:10]**2, val[0][:10], val_[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e0bd06dc-53e8-4061-9094-cd386c97becc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.1310e-03, -1.3311e-04,  5.6899e-04, -1.0748e-03,  2.4993e-03],\n",
       "         [ 2.5153e-02,  2.7871e-03,  1.7440e-02, -7.4674e-04,  1.2350e-01],\n",
       "         [ 1.9880e-03, -2.5313e-05,  3.4438e-04, -9.2611e-05,  3.1835e-03],\n",
       "         [ 7.2101e-03, -1.0660e-02,  1.3801e-03, -1.3957e-03,  5.7525e-03],\n",
       "         [ 6.1774e-03, -3.2167e-03,  5.2106e-04, -1.2708e-03,  7.1740e-03]]),\n",
       " tensor([[-1.1311e-03+0.j, -1.3302e-04+0.j, -5.6890e-04+0.j,  1.0750e-03+0.j],\n",
       "         [-2.5153e-02+0.j,  2.7869e-03+0.j, -1.7440e-02+0.j,  7.4698e-04+0.j],\n",
       "         [-1.9880e-03+0.j, -2.5305e-05+0.j, -3.4451e-04+0.j,  9.2737e-05+0.j],\n",
       "         [-7.2097e-03+0.j, -1.0660e-02+0.j, -1.3800e-03+0.j,  1.3958e-03+0.j],\n",
       "         [-6.1774e-03+0.j, -3.2167e-03+0.j, -5.2106e-04+0.j,  1.2708e-03+0.j]]))"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U[0][:5,:5], vec[0][:5,:4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
