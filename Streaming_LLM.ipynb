{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Experimentation on Streaming LLM with attention sinks.\n",
        "Code taken from https://github.com/mit-han-lab/streaming-llm."
      ],
      "metadata": {
        "id": "PKpwzJFCE5l7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KVQ3qNdaSx6",
        "outputId": "7981103d-4f8e-470c-b966-8c61efb4c161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'streaming-llm'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 104 (delta 27), reused 26 (delta 21), pack-reused 69\u001b[K\n",
            "Receiving objects: 100% (104/104), 39.20 MiB | 60.00 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mit-han-lab/streaming-llm.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers==4.33.0 accelerate datasets evaluate wandb scikit-learn scipy sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I26mid5kbRll",
        "outputId": "f34efdf5-afc3-4751-dea4-9b4e4a3d5f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.33.0\n",
            "  Downloading transformers-4.33.0-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.3)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers==4.33.0)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.33.0)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.32.0-py2.py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=dc579b6f959e158a552564a01c949f760b213e7ec9bb7c29f45bcf83065582bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, sentencepiece, pathtools, smmap, setproctitle, sentry-sdk, safetensors, docker-pycreds, dill, responses, multiprocess, huggingface-hub, gitdb, transformers, GitPython, accelerate, wandb, datasets, evaluate\n",
            "Successfully installed GitPython-3.1.40 accelerate-0.23.0 datasets-2.14.6 dill-0.3.7 docker-pycreds-0.4.0 evaluate-0.4.1 gitdb-4.0.11 huggingface-hub-0.18.0 multiprocess-0.70.15 pathtools-0.1.2 responses-0.18.0 safetensors-0.4.0 sentencepiece-0.1.99 sentry-sdk-1.32.0 setproctitle-1.3.3 smmap-5.0.1 tokenizers-0.13.3 transformers-4.33.0 wandb-0.15.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/streaming-llm/\n",
        "!python setup.py develop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocxdwncNblt2",
        "outputId": "b827c4df-a181-46dd-e613-b8bf6f98eed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/streaming-llm\n",
            "running develop\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running egg_info\n",
            "creating streaming_llm.egg-info\n",
            "writing streaming_llm.egg-info/PKG-INFO\n",
            "writing dependency_links to streaming_llm.egg-info/dependency_links.txt\n",
            "writing top-level names to streaming_llm.egg-info/top_level.txt\n",
            "writing manifest file 'streaming_llm.egg-info/SOURCES.txt'\n",
            "reading manifest file 'streaming_llm.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'streaming_llm.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.10/dist-packages/streaming-llm.egg-link (link to .)\n",
            "Adding streaming-llm 0.0.1 to easy-install.pth file\n",
            "\n",
            "Installed /content/streaming-llm\n",
            "Processing dependencies for streaming-llm==0.0.1\n",
            "Finished processing dependencies for streaming-llm==0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary"
      ],
      "metadata": {
        "id": "otgcrj25eXfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ovy_b749iKZ",
        "outputId": "f411832a-d586-4fa4-82e4-b0052a0897b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export PYTHONPATH=/content/streaming-llm:$PYTHONPATH"
      ],
      "metadata": {
        "id": "0LVl1X81gXpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Namespace(object):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)"
      ],
      "metadata": {
        "id": "hu05nLEZi466"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from streaming_llm.kv_cache import StartRecentKVCache\n",
        "from streaming_llm.utils import parse_args, load\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "args = Namespace(\n",
        "    model_name_or_path='NousResearch/Llama-2-7b-hf',  #NousResearch/Yarn-Llama-2-13b-64k\n",
        "    revision='main',\n",
        "    tokenizer_name_or_path=None,\n",
        "    dataset_name='pg19',\n",
        "    task=None,\n",
        "    split='test',\n",
        "    num_samples=1,\n",
        "    output_dir='outputs/debug',\n",
        "    enable_start_recent_kv_cache=True,\n",
        "    start_size=1,\n",
        "    recent_size=255,\n",
        "    enable_pos_shift=True,\n",
        "    num_eval_tokens=1024 + 32,\n",
        "    repeat_tokens=True,\n",
        "    repeat_step=1,\n",
        ")\n",
        "\n",
        "# data = load_dataset(\n",
        "#     args.dataset_name, args.task, split=args.split, streaming=True\n",
        "# ).take(args.num_samples)\n",
        "\n",
        "# model, tokenizer = load(args.model_name_or_path)"
      ],
      "metadata": {
        "id": "GApG5oJ5gzEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlls = []\n",
        "full_attn_nlls = []\n",
        "sliding_nlls = []\n",
        "offsets = []\n",
        "ppls = []\n",
        "full_attn_ppls = []\n",
        "sliding_ppls = []\n",
        "\n",
        "loss_fn = CrossEntropyLoss(reduction=\"none\")\n",
        "past_key_values = None\n",
        "\n",
        "if args.enable_start_recent_kv_cache:\n",
        "    if \"llama\" in model.config.model_type:\n",
        "        k_seq_dim = v_seq_dim = 2\n",
        "    else:\n",
        "        raise ValueError(f\"got {model.config.model_type}\")\n",
        "\n",
        "if args.enable_pos_shift:\n",
        "    if \"llama\" in model.config.model_type:\n",
        "        from streaming_llm.pos_shift.modify_llama import enable_llama_pos_shift_attention\n",
        "\n",
        "        enable_llama_pos_shift_attention(model)\n",
        "    else:\n",
        "        raise ValueError(f\"got {model.config.model_type}\")\n",
        "\n",
        "\n",
        "os.makedirs(args.output_dir, exist_ok=True)\n",
        "f = open(f\"{args.output_dir}/log.txt\", \"w\")\n",
        "\n",
        "for text in map(lambda x: x['text'], data):\n",
        "    num_eval_tokens = 0\n",
        "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
        "    # tok_idx_offset = 1000\n",
        "    # encodings.input_ids = torch.arange(tok_idx_offset, args.num_eval_tokens + tok_idx_offset, dtype=encodings.input_ids.dtype)[None]\n",
        "\n",
        "    seq_len = encodings.input_ids.size(1)\n",
        "    repeat_toks = args.start_size + args.recent_size\n",
        "    encodings.input_ids = encodings.input_ids[:,:args.num_eval_tokens]\n",
        "    assert repeat_toks < args.num_eval_tokens\n",
        "\n",
        "    for j in range(0, args.num_eval_tokens - 2 * repeat_toks, args.repeat_step):\n",
        "        curr_input_ids = encodings.input_ids[:,:2*repeat_toks + j].clone()\n",
        "        eval_tok_shift = repeat_toks + j\n",
        "        print(f\"step={j//args.repeat_step+1}/{(args.num_eval_tokens - 2 * repeat_toks) // args.repeat_step}\")\n",
        "\n",
        "        # repeat tokens - offset repetition by j\n",
        "        if args.repeat_tokens:\n",
        "            curr_input_ids[:,repeat_toks + j:] = curr_input_ids[:,:repeat_toks]\n",
        "\n",
        "        # sliding-window attn (with recomputation)\n",
        "        pbar = tqdm(range(eval_tok_shift, 2*repeat_toks + j - 1))\n",
        "\n",
        "        #print(f\"evaluating windowed attn on tokens {eval_tok_shift - repeat_toks + 1}-{2*repeat_toks + j}\")\n",
        "\n",
        "        for idx in pbar:\n",
        "            input_ids = curr_input_ids[:, max(0,idx-repeat_toks):idx].to(device)\n",
        "            with torch.no_grad():\n",
        "                #print(f\"windowed: input_ids={input_ids}\")\n",
        "                outputs = model(\n",
        "                    input_ids\n",
        "                )\n",
        "                logits = outputs.logits[:,-1:].view(-1, model.config.vocab_size)\n",
        "                past_key_values = outputs.past_key_values\n",
        "                label = curr_input_ids[:, idx: idx + 1].to(logits.device).view(-1)\n",
        "                #print(f\"windowed: labels={label}\")\n",
        "                neg_log_likelihood = loss_fn(logits, label)\n",
        "                sliding_nlls.append(neg_log_likelihood)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        # full-attn LLM\n",
        "        input_ids = curr_input_ids[:, :2*repeat_toks + j].to(device)\n",
        "        #print(f\"evaluating full attn on tokens {input_ids.shape[1]-repeat_toks+1}-{2*repeat_toks + j}\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids,\n",
        "            )\n",
        "            #print(f\"full-attn: input_ids={input_ids}\")\n",
        "            logits = outputs.logits[:,-repeat_toks-1:-1].view(-1, model.config.vocab_size)\n",
        "            label = input_ids[:,-repeat_toks:].to(logits.device).view(-1)\n",
        "            #print(f\"full-attn: label={label}\")\n",
        "            neg_log_likelihood = loss_fn(logits, label)\n",
        "            full_attn_nlls.append(neg_log_likelihood)\n",
        "\n",
        "        # streaming LLM - process initial cache warmup in parallel\n",
        "        input_ids = curr_input_ids[:, :repeat_toks].to(device)\n",
        "        #print(f\"evaluating streaming on tokens {max(repeat_toks, eval_tok_shift)+1}-{2*repeat_toks + j}\")\n",
        "        kv_cache = StartRecentKVCache(\n",
        "            start_size=args.start_size,\n",
        "            recent_size=args.recent_size,\n",
        "            k_seq_dim=k_seq_dim,\n",
        "            v_seq_dim=v_seq_dim,\n",
        "        )\n",
        "        #print(f\"stream init: input_ids={input_ids}\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True,\n",
        "            )\n",
        "            past_key_values = outputs.past_key_values\n",
        "            if kv_cache is not None:\n",
        "                past_key_values = kv_cache(past_key_values)\n",
        "        if repeat_toks >= eval_tok_shift:\n",
        "            logits = outputs.logits[:,-1:].view(-1, model.config.vocab_size)\n",
        "            label = curr_input_ids[:, repeat_toks : repeat_toks + 1].to(logits.device).view(-1)\n",
        "            #print(f\"stream init: label={label}\")\n",
        "            neg_log_likelihood = loss_fn(logits, label)\n",
        "\n",
        "        # begin streaming\n",
        "        pbar = tqdm(range(repeat_toks, 2*repeat_toks + j - 1))\n",
        "\n",
        "        for idx in pbar:\n",
        "            input_ids = curr_input_ids[:, idx : idx + 1].to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids,\n",
        "                    past_key_values=past_key_values,\n",
        "                    use_cache=True,\n",
        "                )\n",
        "                #print(f\"streaming: input_ids={input_ids}\")\n",
        "                logits = outputs.logits.view(-1, model.config.vocab_size)\n",
        "                past_key_values = outputs.past_key_values\n",
        "                label = curr_input_ids[:, idx + 1 : idx + 2].to(logits.device).view(-1)\n",
        "                neg_log_likelihood = loss_fn(logits, label)\n",
        "                if kv_cache is not None:\n",
        "                    past_key_values = kv_cache(past_key_values)\n",
        "            if idx >= eval_tok_shift:\n",
        "                nlls.append(neg_log_likelihood)\n",
        "                #print(f\"streaming: label={label}\")\n",
        "            pbar.set_description(\n",
        "                f\"nll: {neg_log_likelihood.item():.2f}, ppl: {torch.exp(neg_log_likelihood).item():.2f}\"\n",
        "            )\n",
        "            #print(neg_log_likelihood.item(), file=f, flush=True)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        offsets.append(j)\n",
        "        ppls.append(torch.exp(torch.cat(nlls).mean()).item())\n",
        "        full_attn_ppls.append(torch.exp(torch.cat(full_attn_nlls).mean()).item())\n",
        "        sliding_ppls.append(torch.exp(torch.stack(sliding_nlls).mean()).item())\n",
        "\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNHSU5dQ6dIQ",
        "outputId": "50730c3a-970c-4db2-9efc-3b225026f09f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=1/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:11<00:00, 23.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.17, ppl: 1.18: 100%|██████████| 255/255 [00:11<00:00, 22.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=2/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:11<00:00, 22.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.25, ppl: 1.28: 100%|██████████| 256/256 [00:11<00:00, 21.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=3/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.26, ppl: 1.30: 100%|██████████| 257/257 [00:11<00:00, 21.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=4/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.28, ppl: 1.32: 100%|██████████| 258/258 [00:11<00:00, 22.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=5/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.26, ppl: 1.30: 100%|██████████| 259/259 [00:11<00:00, 22.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=6/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.26, ppl: 1.30: 100%|██████████| 260/260 [00:11<00:00, 22.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=7/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.26, ppl: 1.30: 100%|██████████| 261/261 [00:11<00:00, 22.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=8/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.26, ppl: 1.30: 100%|██████████| 262/262 [00:11<00:00, 22.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=9/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.26, ppl: 1.30: 100%|██████████| 263/263 [00:11<00:00, 21.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=10/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.26, ppl: 1.30: 100%|██████████| 264/264 [00:11<00:00, 22.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=11/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.27, ppl: 1.31: 100%|██████████| 265/265 [00:12<00:00, 22.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=12/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.27, ppl: 1.31: 100%|██████████| 266/266 [00:11<00:00, 22.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=13/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.27, ppl: 1.31: 100%|██████████| 267/267 [00:12<00:00, 22.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=14/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.27, ppl: 1.31: 100%|██████████| 268/268 [00:12<00:00, 22.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=15/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.27, ppl: 1.31: 100%|██████████| 269/269 [00:12<00:00, 22.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=16/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.26, ppl: 1.30: 100%|██████████| 270/270 [00:12<00:00, 22.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=17/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.26, ppl: 1.30: 100%|██████████| 271/271 [00:12<00:00, 21.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=18/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.27, ppl: 1.31: 100%|██████████| 272/272 [00:12<00:00, 22.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=19/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.27, ppl: 1.31: 100%|██████████| 273/273 [00:12<00:00, 22.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=20/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.27, ppl: 1.31: 100%|██████████| 274/274 [00:12<00:00, 21.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=21/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.27, ppl: 1.31: 100%|██████████| 275/275 [00:12<00:00, 22.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=22/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.27, ppl: 1.31: 100%|██████████| 276/276 [00:12<00:00, 21.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=23/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.28, ppl: 1.32: 100%|██████████| 277/277 [00:12<00:00, 21.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=24/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:11<00:00, 22.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.29, ppl: 1.33: 100%|██████████| 278/278 [00:12<00:00, 21.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=25/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:11<00:00, 22.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.28, ppl: 1.33: 100%|██████████| 279/279 [00:12<00:00, 21.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=26/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:10<00:00, 23.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.29, ppl: 1.34: 100%|██████████| 280/280 [00:12<00:00, 22.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=27/544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 255/255 [00:11<00:00, 23.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 1.06, ppl: 2.89:  80%|████████  | 225/281 [00:10<00:02, 22.44it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_attn_ppls)\n",
        "print(ppls)\n",
        "print(sliding_ppls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlEJC-2-8MFc",
        "outputId": "95935019-8aa1-4db7-9ee5-3045f15ae8d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.402909755706787, 1.4052437543869019, 1.404779076576233, 1.4090758562088013, 1.4134364128112793, 1.4172700643539429, 1.4204535484313965, 1.4195637702941895, 1.4187251329421997, 1.4263849258422852, 1.4335627555847168]\n",
            "[2.718900203704834, 4.016863822937012, 4.5563812255859375, 4.914188385009766, 5.1250128746032715, 5.25825309753418, 5.360547065734863, 5.44130802154541, 5.530566692352295, 5.591329097747803, 5.649539947509766]\n",
            "[5.766645908355713, 6.2545881271362305, 6.386405944824219, 6.479013442993164, 6.538914680480957, 6.567478179931641, 6.552601337432861, 6.5352325439453125, 6.539529800415039, 6.537642002105713, 6.588767051696777]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape, label.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r61H71Qr48DR",
        "outputId": "bdb652a9-5c6d-4fae-b8b2-e04f9712776d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 32000]), torch.Size([1]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "curr_input_ids[:, max(0,idx-repeat_toks):idx+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LocXK5r04Zj",
        "outputId": "2f539951-6260-413e-9ca3-88f18b5a8490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1006, 1007, 1000, 1001, 1002, 1003, 1004, 1005, 1006]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "curr_input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZYCp3g_qCdv",
        "outputId": "f8fac1ba-7982-4294-dfe7-2a498e37099b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1000, 1001, 1002, 1003,\n",
              "         1004, 1005, 1006, 1007]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(range(0, args.num_eval_tokens - 2 * repeat_toks, args.repeat_step))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQN3rTxxmmFC",
        "outputId": "06b09c4a-3809-4dfd-eed5-695df7fb10cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 128, 256, 384, 512, 640, 768, 896]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ppls)\n",
        "print(full_attn_ppls)\n",
        "print(sliding_ppls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PeRayAwl9-4",
        "outputId": "38c23b61-2b78-4050-f4c6-f1a6c28fc7b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.509871244430542, 84.67305755615234, 396.76214599609375, 1057.8154296875, 1654.1778564453125, 2369.919677734375, 3034.802001953125, 3964.217529296875, 4848.52197265625, 5820.080078125, 6702.83349609375, 7562.923828125, 8321.2314453125, 9160.236328125, 9860.6015625, 10629.921875]\n",
            "[6.709757328033447, 6.241941928863525, 6.940530300140381, 6.778741359710693, 6.974102020263672, 7.429309368133545, 6.702706336975098, 6.80616569519043, 6.346508502960205, 6.005157470703125, 5.8393731117248535, 5.620218753814697, 5.47456693649292, 5.294277667999268, 5.3683013916015625, 5.2660627365112305]\n",
            "[6717.744140625, 12659.8623046875, 18004.517578125, 18762.900390625, 21132.140625, 21226.466796875, 20000.583984375, 22740.46875, 23553.732421875, 25710.1015625, 26982.345703125, 27379.380859375, 28355.13671875, 29006.0859375, 30106.89453125, 30694.435546875]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from streaming_llm.kv_cache import StartRecentKVCache\n",
        "from streaming_llm.utils import parse_args, load\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "args = Namespace(\n",
        "    model_name_or_path='NousResearch/Llama-2-7b-hf',  #NousResearch/Yarn-Llama-2-13b-64k\n",
        "    revision='main',\n",
        "    tokenizer_name_or_path=None,\n",
        "    dataset_name='pg19',\n",
        "    task=None,\n",
        "    split='test',\n",
        "    num_samples=1,\n",
        "    output_dir='outputs/debug',\n",
        "    enable_start_recent_kv_cache=True,\n",
        "    start_size=1,\n",
        "    recent_size=7,\n",
        "    enable_pos_shift=True,\n",
        "    num_eval_tokens=16,\n",
        "    repeat_tokens=False,\n",
        ")\n",
        "\n",
        "# data = load_dataset(\n",
        "#     args.dataset_name, args.task, split=args.split, streaming=True\n",
        "# ).take(args.num_samples)\n",
        "\n",
        "#model, tokenizer = load(args.model_name_or_path)\n",
        "\n",
        "nlls = []\n",
        "full_attn_nlls = []\n",
        "sliding_nlls = []\n",
        "\n",
        "loss_fn = CrossEntropyLoss(reduction=\"none\")\n",
        "past_key_values = None\n",
        "\n",
        "if args.enable_start_recent_kv_cache:\n",
        "    if \"llama\" in model.config.model_type:\n",
        "        k_seq_dim = v_seq_dim = 2\n",
        "    else:\n",
        "        raise ValueError(f\"got {model.config.model_type}\")\n",
        "    kv_cache = StartRecentKVCache(\n",
        "        start_size=args.start_size,\n",
        "        recent_size=args.recent_size,\n",
        "        k_seq_dim=k_seq_dim,\n",
        "        v_seq_dim=v_seq_dim,\n",
        "    )\n",
        "else:\n",
        "    kv_cache = None\n",
        "\n",
        "if args.enable_pos_shift:\n",
        "    if \"llama\" in model.config.model_type:\n",
        "        from streaming_llm.pos_shift.modify_llama import enable_llama_pos_shift_attention\n",
        "\n",
        "        enable_llama_pos_shift_attention(model)\n",
        "    else:\n",
        "        raise ValueError(f\"got {model.config.model_type}\")\n",
        "\n",
        "\n",
        "os.makedirs(args.output_dir, exist_ok=True)\n",
        "f = open(f\"{args.output_dir}/log.txt\", \"w\")\n",
        "\n",
        "num_eval_tokens = 0\n",
        "for text in map(lambda x: x['text'], data):\n",
        "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
        "    encodings.input_ids = torch.arange(0, 8, dtype=encodings.input_ids.dtype).repeat(2)[None]\n",
        "\n",
        "    #print(encodings.input_ids[:, :10])\n",
        "\n",
        "    seq_len = encodings.input_ids.size(1)\n",
        "    eval_tok_shift = args.start_size + args.recent_size\n",
        "\n",
        "    # repeat tokens\n",
        "    if args.repeat_tokens:\n",
        "        encodings.input_ids[:,eval_tok_shift:2*eval_tok_shift] = encodings.input_ids[:,:eval_tok_shift]\n",
        "\n",
        "    #print(f\"seq_len: {seq_len}\")\n",
        "\n",
        "    # sliding-window attn (with recomputation)\n",
        "    pbar = tqdm(range(eval_tok_shift, args.num_eval_tokens - 1))\n",
        "\n",
        "    for idx in pbar:\n",
        "        input_ids = encodings.input_ids[:, max(0,idx-eval_tok_shift):idx+1].to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids\n",
        "            )\n",
        "            logits = outputs.logits[:,-1:].view(-1, model.config.vocab_size)\n",
        "            past_key_values = outputs.past_key_values\n",
        "            label = encodings.input_ids[:, idx + 1 : idx + 2].to(logits.device).view(-1)\n",
        "            print(f\"windowed: input_ids={input_ids}\")\n",
        "            print(f\"windowed: label={label}\")\n",
        "            neg_log_likelihood = loss_fn(logits, label)\n",
        "            sliding_nlls.append(neg_log_likelihood)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    # full-attn LLM\n",
        "    input_ids = encodings.input_ids[:, :args.num_eval_tokens].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids,\n",
        "        )\n",
        "        logits = outputs.logits[:,eval_tok_shift:-1].view(-1, model.config.vocab_size)\n",
        "        label = input_ids[:,eval_tok_shift+1:].to(logits.device).view(-1)\n",
        "        print(f\"full: input_ids={input_ids}\")\n",
        "        print(f\"full: label={label}\")\n",
        "        neg_log_likelihood = loss_fn(logits, label)\n",
        "        full_attn_nlls.append(neg_log_likelihood)\n",
        "\n",
        "    # streaming LLM - process initial cache warmup in parallel\n",
        "    input_ids = encodings.input_ids[:, :eval_tok_shift].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=True,\n",
        "        )\n",
        "        print(f\"stream init: input_ids={input_ids}\")\n",
        "        past_key_values = outputs.past_key_values\n",
        "        if kv_cache is not None:\n",
        "            past_key_values = kv_cache(past_key_values)\n",
        "\n",
        "    # begin streaming\n",
        "    pbar = tqdm(range(eval_tok_shift, args.num_eval_tokens - 1))\n",
        "\n",
        "    for idx in pbar:\n",
        "        input_ids = encodings.input_ids[:, idx : idx + 1].to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True,\n",
        "            )\n",
        "            logits = outputs.logits.view(-1, model.config.vocab_size)\n",
        "            past_key_values = outputs.past_key_values\n",
        "            label = encodings.input_ids[:, idx + 1 : idx + 2].to(logits.device).view(-1)\n",
        "            print(f\"streaming: input_ids={input_ids}\")\n",
        "            neg_log_likelihood = loss_fn(logits, label)\n",
        "            if kv_cache is not None:\n",
        "                past_key_values = kv_cache(past_key_values)\n",
        "        if idx >= eval_tok_shift:\n",
        "            nlls.append(neg_log_likelihood)\n",
        "            print(f\"streaming: label={label}\")\n",
        "        pbar.set_description(\n",
        "            f\"nll: {neg_log_likelihood.item():.2f}, ppl: {torch.exp(neg_log_likelihood).item():.2f}\"\n",
        "        )\n",
        "        print(neg_log_likelihood.item(), file=f, flush=True)\n",
        "        num_eval_tokens += 1\n",
        "        if args.num_eval_tokens is not None and num_eval_tokens >= args.num_eval_tokens:\n",
        "            break\n",
        "\n",
        "    if args.num_eval_tokens is not None and num_eval_tokens >= args.num_eval_tokens:\n",
        "        break\n",
        "\n",
        "f.close()\n",
        "\n",
        "ppl = torch.exp(torch.stack(nlls).mean()).item()\n",
        "full_attn_ppl = torch.exp(torch.cat(full_attn_nlls).mean()).item()\n",
        "sliding_ppl = torch.exp(torch.stack(sliding_nlls).mean()).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFtjJeu26BNL",
        "outputId": "49a76fd9-8c4e-4cc3-c7d7-e968e4d1e2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StartRecentKVCache: 1, 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 3/7 [00:00<00:00, 22.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "windowed: input_ids=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 0]], device='cuda:0')\n",
            "windowed: label=tensor([1], device='cuda:0')\n",
            "windowed: input_ids=tensor([[1, 2, 3, 4, 5, 6, 7, 0, 1]], device='cuda:0')\n",
            "windowed: label=tensor([2], device='cuda:0')\n",
            "windowed: input_ids=tensor([[2, 3, 4, 5, 6, 7, 0, 1, 2]], device='cuda:0')\n",
            "windowed: label=tensor([3], device='cuda:0')\n",
            "windowed: input_ids=tensor([[3, 4, 5, 6, 7, 0, 1, 2, 3]], device='cuda:0')\n",
            "windowed: label=tensor([4], device='cuda:0')\n",
            "windowed: input_ids=tensor([[4, 5, 6, 7, 0, 1, 2, 3, 4]], device='cuda:0')\n",
            "windowed: label=tensor([5], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 22.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "windowed: input_ids=tensor([[5, 6, 7, 0, 1, 2, 3, 4, 5]], device='cuda:0')\n",
            "windowed: label=tensor([6], device='cuda:0')\n",
            "windowed: input_ids=tensor([[6, 7, 0, 1, 2, 3, 4, 5, 6]], device='cuda:0')\n",
            "windowed: label=tensor([7], device='cuda:0')\n",
            "full: input_ids=tensor([[0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7]], device='cuda:0')\n",
            "full: label=tensor([1, 2, 3, 4, 5, 6, 7], device='cuda:0')\n",
            "stream init: input_ids=tensor([[0, 1, 2, 3, 4, 5, 6, 7]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 11.26, ppl: 77334.80:   0%|          | 0/7 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "streaming: input_ids=tensor([[0]], device='cuda:0')\n",
            "streaming: label=tensor([1], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 2.48, ppl: 11.92:  43%|████▎     | 3/7 [00:00<00:00, 21.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "streaming: input_ids=tensor([[1]], device='cuda:0')\n",
            "streaming: label=tensor([2], device='cuda:0')\n",
            "streaming: input_ids=tensor([[2]], device='cuda:0')\n",
            "streaming: label=tensor([3], device='cuda:0')\n",
            "streaming: input_ids=tensor([[3]], device='cuda:0')\n",
            "streaming: label=tensor([4], device='cuda:0')\n",
            "streaming: input_ids=tensor([[4]], device='cuda:0')\n",
            "streaming: label=tensor([5], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 0.60, ppl: 1.81:  86%|████████▌ | 6/7 [00:00<00:00, 21.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "streaming: input_ids=tensor([[5]], device='cuda:0')\n",
            "streaming: label=tensor([6], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nll: 1.24, ppl: 3.46: 100%|██████████| 7/7 [00:00<00:00, 21.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "streaming: input_ids=tensor([[6]], device='cuda:0')\n",
            "streaming: label=tensor([7], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# partial forward propagation of information from remaining keys in rolling cache\n",
        "sliding_ppl, ppl, full_attn_ppl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P3gmSGv9ixF",
        "outputId": "41ca741d-8967-46d8-8408-5e62140eb879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4.56148624420166, 2.8147730827331543, 1.184415340423584)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# without repeat\n",
        "ppl, full_attn_ppl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfZSKQ8C8GGS",
        "outputId": "e6dccec7-d416-437d-c30c-c00a05bfbf92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5.599760055541992, 5.257948398590088)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with repeat\n",
        "ppl, full_attn_ppl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R25cgb2Y60Nb",
        "outputId": "4a72c09a-3738-4e13-ba0f-8847e582fbf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.5983119010925293, 1.184415340423584)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}